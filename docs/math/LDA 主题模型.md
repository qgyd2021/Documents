## LDA 主题模型

https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf





```text
图床
https://sm.ms/
```





### 摘要

我们描述了 `latent Dirichlet allocation` 潜在狄利克雷分配 (LDA), 一种用于收集离散数据 (如文本语料库) 的生成概率模型. LDA 是一个三层分层贝叶斯模型, 其中集合的每个项目都被建模为基础主题集的有限混合. 反过来, 每个主题都被建模为一组基础主题概率上的无限混合. 在文本建模的上下文中, 主题概率提供了文档的明确表示. 我们提出了基于变分方法的有效近似推理技术和经验贝叶斯参数估计的 EM 算法. 与一元模型和概率 LSI 模型的混合相比, 我们报告了文档建模, 文本分类和协同过滤方面的结果. 



### 1. 介绍

在本文中, 我们考虑对文本语料库和其他离散数据集合进行建模的问题. 目标是找到对集合成员的简短描述, 以便能够有效处理大型集合, 同时保留对分类, 新颖性检测, 总结以及相似性和相关性判断等基本任务有用的基本统计关系. 信息检索 (IR) 领域的研究人员在这个问题上取得了重大进展 (Baeza-Yates and Ribeiro-Neto, 1999). IR 研究人员为文本语料库提出的基本方法, 一种成功部署在现代互联网搜索引擎中的方法, 将语料库中的每个文档都简化为一个实数向量, 每个实数代表计数的比率. 在流行的 TF-IDF 方案 (Salton and McGill, 1983) 中, 选择 "words" 或 "terms" 的基本词汇表, 并且对语料库中的每个文档, 由每个单词的出现次数形成计数. 当适当的归一化之后, 将这个词频计数与逆文档频率计数进行比较, 逆向文档频率计数测量一个单词在整个语料库中出现的次数 (通常在对数尺度上, 并再次进行适当的归一化). 最终结果是一个逐个文档矩阵 $X$, 其列包含语料库中每个文档的 TF-IDF 值. 因此, TF-IDF 方案将任意长度的文档减少为固定长度的数字列表. 

虽然 TF-IDF 缩减具有一些吸引人的特性, 特别是在它对集合中的文档有区别的词集的基本识别方面, 该方法还提供了相对少量的描述长度缩减, 并且几乎没有揭示或文档内统计结构. 为了解决这些缺点, IR 研究人员提出了其他几种降维技术, 最值得注意的是潜在语义索引 (LSI) (Deerwester et al., 1990). LSI 从捕获了集合中的大部分方差的 TF-IDF 特征的 $X$ 矩阵做奇异值分解. 这种方法可以在大型集合中实现显著压缩. 此外,  Deerwester et al.  认为 LSI 的派生特征是 TF-IDF 特征的线性组合, 可以捕获基本语言概念的某些方面, 例如同义和多义词. 

```text
TF-IDF
1. 从所有文档中分词. 统计单词出现次数,将出现次数太少的单词排除;统计单词出现的文档数,按文档比例,将出现比率太低或太高的单词排除. 得到词汇表, 也称作特征. 
2. 统计TF, 即单词频率. 一种实现是直接统计词出现的次数(长的文档,单词多,相对单词出现的次数也会比较多,可能不公平); 一种是按单词在文档中出现的概率来统计(即对单词出现次数与总词数做归一化).
3. 统计IDF, 即逆文档频率. 用总文档数除以出现过该单词的文档数量. 出现该单词的文档越少, 说明该单词越重要, 越有代表性. 
4. 对 TF 和 IDF 作 log 取对数, 降低最大值的权重. 
5. TF 与 IDF 相乘, 得到 TF-IDF, 即每个词在每个文档中的一个数值, 得到该文档的数值化表示 (向量).

应用: 
1. 通过每个文档的向量, 可对文档聚类. 找出相似文档. 也可通过向量相似度判断文档相似度. 
2. 如果事先已经知道文档的类别. 则在统计之前, 可将相同类别的文档合并成一个文档. 再计算 TF-IDF, 得向该类别文档的向量, 各数值可表示该词对该文档的重要性. 当得到一篇新的文档时, 可结合文档中各词出现的次数, 累加各词在各类别文档向量中对应位置的 TF-IDF 数值, 最后哪一个类别对应的数值最大, 就判断该文档属于哪个类别. 
```



为了证实有关 LSI 的说法, 并研究其相对优势和劣势, 开发文本语料库的生成概率模型并研究 LSI 从数据中恢复生成模型的各个方面的能力是有用的 (Papadimitriou et al., 1998). 然而, 给定文本的生成模型, 尚不清楚为什么应该采用 LSI 方法,因为,  可以尝试更直接地进行, 使用最大似然法或贝叶斯方法将模型拟合到数据中. 

```text
https://www.jianshu.com/p/a3d78abcff51

LSI (Latent Semantic Indexing) 也称为 LSA (Latent Semantic Analysis). 
1. 计算文档的 TFIDF 值矩阵. 
2. 对矩阵做奇异值分解. 
3. 文档由主题组成, 主题由词义组成, 词义由词语组成. 

```



Hofmann (1999) 在这方面向前迈出了重要一步, 他提出了概率 LSI (pLSI) 模型, 也称为 `aspect model` 方面模型, 作为 LSI 的替代方案. 我们在第 4.3 节中详细描述的 pLSI 方法将文档中的每个单词建模为来自混合模型的样本, 其中混合成分是多项随机变量, 可以被视为 "主题" 的表示. 因此, 每个单词都是从单个主题生成的, 并且文档中的不同单词可以从不同的主题生成. 每个文档都表示为这些混合成分的混合比例列表, 从而降低为概率分布在一组固定的主题上. 此分布是与文档相关的 "简化描述". 虽然 Hofmann 的工作是朝着文本概率建模迈出的有用一步, 但它并不完整, 因为它没有提供文档级别的概率模型. 

在 pLSI 中, 每个文档都表示为一个数字列表 (主题的混合比例), 并且这些数字没有生成概率模型. 这导致了几个问题: (1) 模型中的参数数量随着语料库的大小线性增长, 这会导致严重的过拟合问题, 以及 (2) 不清楚如何为训练集之外的文档分配概率. 为了了解如何超越 pLSI, 让我们考虑包括 LSI 和 pLSI 在内的降维方法类的基本概率假设. 所有这些方法都基于 "词袋" 假设, 文档中的单词顺序可以忽略. 在概率论的语言中, 这是对文档中单词可交换性的假设 (Aldous, 1985). 此外, 虽然很少正式声明, 但这些方法也假设文件是可交换的; 语料库中文档的具体排序也可忽略. 

```
https://www.jianshu.com/p/e2fa2575b64e
https://blog.csdn.net/mch2869253130/article/details/108607644

pLSI (Probabilistic Latent Semantic Indexing)


```



由 de Finetti (1990) 提出的经典表示定理确定任何可交换随机变量的集合都具有混合分布的表示, 通常是无限混合. 因此, 如果我们希望考虑文档和单词的可交换表示, 我们需要考虑捕获单词和文档的可交换性的混合模型. 这种思路导致了我们在当前论文中提出的潜在狄利克雷分配 (LDA) 模型. 

需要强调的是, 可交换性假设并不等同于随机变量独立分布的假设. 相反, 可交换性本质上可以解释为 "条件独立且同分布", 其中条件是相对于概率分布的潜在隐参数. 在有条件的情况下, 随机变量的联合分布很简单并且是因式分解的, 而在潜在参数的边缘上, 联合分布可能很复杂. 因此, 虽然可交换性假设显然是文本建模领域的主要简化假设, 其主要理由是它导致计算高效的方法, 但可交换性假设不一定会导致仅限于简单频率的方法计数或线性操作. 我们的目标是在当前的论文中证明, 通过认真对待 de Finetti 定理, 我们可以通过以下方式捕获重要的文档内统计结构混合分布. 

还值得注意的是, 可交换性的基本概念有大量推广, 包括各种形式的部分可交换性, 并且表示定理也适用于这些情况 (Diaconis, 1988). 因此, 虽然我们在当前论文中讨论的工作侧重于简单的 "词袋" 模型, 这会导致单个词 (unigrams) 的混合分布, 但我们的方法也适用于涉及更大结构的混合的更丰富的模型单位, 例如 ngram 或段落. 

本文的结构如下. 在第 2 节中, 我们介绍了基本符号和术语. LDA 模型在第 3 节中介绍, 并在第 4 节中与相关的潜在变量模型进行比较. 我们在第 5 节中讨论 LDA 的推理和参数估计. 第 6 节提供了将 LDA 拟合到数据的说明性示例. 本文中的经验结果第 7 节介绍了建模, 文本分类和协同过滤. 最后, 第 8 节价绍了我们的结论. 



### 2. 符号和术语

我们在整篇论文中使用文本集合的语言, 指的是 "单词", "文档" 和 "语料库" 等实体. 这很有用, 因为它有助于引导直觉, 特别是我们引入旨在捕获抽象概念 (如主题) 的潜在变量时. 然而, 值得注意的是, LDA 模型不一定与文本相关联, 并且可以应用于涉及数据集合的其他问题, 包括来自协同过滤, 基于内容的图像检索和生物信息学等领域的数据. 事实上, 在第 7.3 节中, 我们展示了协同过滤领域的实验结果. 

形式上, 我们定义了以下术语: 

* 一个词是离散数据的基本单位, 定义为来自由 $\{ 1, \cdots, V \}$. 我们使用单位基向量来表示单词, 该向量的单个分量等于 $1$, 所有其他分量等 $0$. 因此, 使用上标表示组件, 词汇表中的第 $v$ 个单词由 $V$ 向量 $w$ 表示, 使得 $w^{v} = 1$ 和 $w^{u} = 0$, $u \ne v$ . 
* 文档是由 $n$ 个单词组成的序列, 记为  $ \mathbf{w}= ( w_{1}, w_{2}, \cdots, w_{N} )$, 其中 $w_{n}$ 是序列中的第 $n$ 个单词. 
* 语料库是 $M$ 个文档的集合, 记为 $D = \{ \mathbf{w}_{1}, \mathbf{w}_{2}, \cdots , \mathbf{w}_{M} \}$. 

我们希望找到一个语料库的概率模型, 它不仅为语料库的成员分配高概率, 而且还为其他 "相似" 文档分配高概率. 



### 3. Latent Dirichlet allocation 潜在狄利克雷分配

潜在狄利克雷分配 (LDA) 是语料库的生成概率模型. 基本思想是将文档表示为潜在主题的随机混合, 其中每个主题的特征在于单词的分布. 

 LDA 对语料库 $D$ 中的每个文档 $\mathbf{w}$ 假设以下生成过程. 

1. 选择 $N \sim \text{Poisson} (\xi) $ 泊松分布. 

2. 选择 $\theta \sim \text{Dir} (\alpha)$. 

3. 对于每个 $N$ words $w_{n}$: 

   (a) 选择一个主题 $\mathbf{z}_{n} \sim \text{Multinomial} (\theta)$. 

   (b) 从 $p(w_{n} | \mathbf{z}_{n}, \beta)$ 选择一个词 $w_{n}$, 以主题 $\mathbf{z}_{n}$ 为条件的多项式概率. 

在这个基本模型中做了几个简化的假设, 我们在后续部分中删除了其中一些假设. 首先, 假设 Dirichlet 分布的维数 $k$  (以及主题变量 $\mathbf{z}$ 的维数) 已知且固定. 其次, 单词概率由下式参数化 $k \times V$ 矩阵 $\beta$ 其中 $\beta_{ij} = p (w^{j} = 1 | \mathbf{z}^{i} = 1)$, 现在我们将其视为要估计的固定数量. 最后, 泊松假设对于接下来的任何事情都不是关键的, 并且可以根据需要命名用更现实的文档长度分布. 此外, 请注意 $N$ 独立于所有其也数据生成变量 ($\theta$ 和 $\mathbf{z}$). 因此它是一个辅助变量, 我们通常会在后续开发中忽略它的随机性. 一个  $k$ 维 Dirichlet 随机变量 $\theta$ 可以在 $\text{(k-1)-simplex}$ (如果 $\theta_{i} \ge 0$, $\sum_{i=1}^{k}{\theta_{i}} = 1$, $\text{(k-1)-simplex}$ 包含一个 $k$ 向量 $\theta$ ), 并且在这个 simplex 单纯形上具有以下概率密度: 

$$\begin{aligned} p(\theta | \alpha) = \frac{\Gamma (\sum_{i=1}^{k}{\alpha_{i}})}{\prod_{i=1}^{k}{\Gamma}(\alpha_{i})} \theta_{1}^{\alpha_{1} - 1} \cdots \theta_{k}^{\alpha_{k}-1}, \quad (1) \end{aligned}$$ 

其中, 参数 $\alpha$ 是带有组件 $\alpha_{i} \gt 0$ 的 $k$ 向量, 其中 $\Gamma (x)$ 是 $\text{Gamma}$ 函数. Dirichlet 是单纯形上的一种方便分布, 它属于指数族, 具有有限维的充分统计量, 并且与多项分布共轭. 在第 5 节中, 这些属性将促进 LDA 推理和参数估计算法的开发. 

给定参数 $\alpha$ 和 $\beta$, 主题混合 $\theta$, 一组 $N$ 主题 $\mathbf{z}$ 和一组 $N$ 个词 $w$ 的联合分布由下式给出: 

$$\begin{aligned} p (\theta, \mathbf{z}, \mathbf{w} | \alpha, \beta) = p (\theta | \alpha) \prod _{n=1}^{N}{p (z_{n} | \theta) p(w_{n} |ｚ_{n}, \beta)}, \quad (2) \end{aligned}$$ 

我们将 LDA 模型中的潜在多项式变量称为主题, 以利用面向文本的直觉, 但我们没有对这些潜在变量提出认识论主张, 超出了它们在表示词集上的概率分布方面的效用. 

![image.png](https://s2.loli.net/2022/06/12/WFPHfwYOyBZQ2ok.png)



其中 $p(z_{n} | \theta)$ 是唯一 $i$ 的简单 $\theta_{i}$ , 使得 $z_{n}^{i} = 1$. 在 $\theta$ 上积分并在 $z$ 上求和, 我们得到文档的边缘分布: 

$$\begin{aligned} p (\mathbf{w} | \alpha , \beta) = \int p (\theta | \alpha) \left( \begin{aligned} \prod_{n=1}^{N}{\sum_{z_{n}} {p (z_{n} | \theta) p (w_{n} | z_{n}, \beta)}} \end{aligned} \right) \text{d}\theta . \quad (3) \end{aligned}$$ 



最后, 取单个文档的边际概率的乘积, 我们得到一个语料库的概率: 

$$\begin{aligned} p (D | \alpha , \beta) = \prod_{d=1}^{M} \int p (\theta_{d} | \alpha) \left( \begin{aligned} \prod_{n=1}^{N_{d}}{\sum_{z_{dn}} {p (z_{dn} | \theta_{d}) p (w_{dn} | z_{dn}, \beta)}} \end{aligned} \right) \text{d}\theta_{d} . \end{aligned}$$ 



LDA 模型在图 1 中表示为概率图形模型. 如图所示, LDA 表示具有三个层次. 参数 $\alpha$ 和 $\beta$ 是语料库级别的参数, 假设在生成语料库的过程中被采样一次. 变量 $\theta_{d}$ 是文档级变量, 每个文档采样一次. 最后, 变量 $z_{dn}$ 和 $w_{dn}$ 是词级变量, 对每个文档中的每个词采样一次. 

将 LDA 与简单的 Dirichlet 多项式聚类模型区分开来很重要. 经典的聚类模型将涉及一个两级模型, 其中对语料库进行一次 Dirichlet 采样, 为语料库中的每个文档选择一次多项聚类变量, 并以集群变量为条件为文档选择一组词. 与许多聚类模型一样, 这种模型将文档限制为单个主题相关联. 另一方面, LDA 涉及三个层次, 特别是主题节点在文档中被重复采样. 在此模型下, 文档可以与多个主题相关联. 与图 1 所示结构相似的结构通常在贝叶斯统计建模中进行研究, 其中它们被称为分层模 (Gelman et al., 1995), 或者更准确地说是条件独立的分层模型 (Kass and Steffey, 1989). 这种模型通常也被称为参数经验贝叶斯模型, 这个术语不仅指特定的模型结构, 还指用于估计模型中参数的方法 (Morris, 1983). 实际上, 正如我们在第 5 节中讨论的那样, 我们采用经验贝叶斯方法来估计 LDA 的简单实现中的参数, 例如 $\alpha$ 和 $\beta$ , 但我们也考虑更全面的贝叶斯方法. 



#### 3.1 LDA 和可交换性





























































