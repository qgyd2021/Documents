## 特征值分解



参考链接: 

```text
这个人的博客讲一些数学原理, 讲得挺好的. 
https://blog.csdn.net/guoziqing506/article/details/80540323
```





###特征值分解

#### 定义

设矩阵 $A$, 有 $y = Ax$, $A$ 是从 $x$ 变换到 $y$ 的变换矩阵. 

是否存在一种向量 $x$, 使得 $Ax = \lambda x = y$. 

特征向量的定义: 设 $A$ 是 $n$ 阶矩阵, 如果数 $\lambda$ 和 $n$ 维非零向量 $x$ 使关系式: $Ax = \lambda x$ 成立, 那么, 这样的数 $\lambda$ 称为矩阵 $A$ 的特征值, 非零向量 $x$ 称为 $A$ 的对应于特征值 $\lambda$ 的特征向量. 

推导: 

$$\begin{aligned} Ax &= \lambda x \\ \left[
\begin{matrix} a_{11} & a_{12} & a_{13} & ... & a_{1n} \\ a_{21} & a_{22} & a_{23} & ... & a_{2n} \\ a_{31} & a_{32} & a_{33} & ... & a_{3n} \\ ... & ... & ... & ... & ... \\ a_{n1} & a_{n2} & a_{n3} & ... & a_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} x_{1} \\ x_{2} \\ x_{3} \\ ... \\ x_{n} \end{matrix}
\right] &= \left[
\begin{matrix} \lambda x_{1} \\ \lambda x_{2} \\ \lambda x_{3} \\ ... \\ \lambda x_{n} \end{matrix}
\right] \\ \left[
\begin{matrix} a_{11}x_{1} + a_{12}x_{2} + a_{13}x_{3} + ... + a_{1n}x_{n} \\ a_{21}x_{1} + a_{22}x_{2} + a_{23}x_{3} + ... + a_{2n}x_{n} \\ a_{31}x_{1} + a_{32}x_{2} + a_{33}x_{3} + ... + a_{3n}x_{n} \\ ... \\ a_{n1}x_{1} + a_{n2}x_{2} + a_{n3}x_{3} + ... + a_{nn}x_{n} \end{matrix}
\right] &= \left[
\begin{matrix} \lambda x_{1} \\ \lambda x_{2} \\ \lambda x_{3} \\ ... \\ \lambda x_{n} \end{matrix}
\right] \\ \left[
\begin{matrix} (a_{11}-\lambda)x_{1} + a_{12}x_{2} + a_{13}x_{3} + ... + a_{1n}x_{n} \\ a_{21}x_{1} + (a_{22}-\lambda)x_{2} + a_{23}x_{3} + ... + a_{2n}x_{n} \\ a_{31}x_{1} + a_{32}x_{2} + (a_{33}-\lambda)x_{3} + ... + a_{3n}x_{n} \\ ... \\ a_{n1}x_{1} + a_{n2}x_{2} + a_{n3}x_{3} + ... + (a_{nn}-\lambda)x_{n} \end{matrix}
\right] &= \left[
\begin{matrix} 0 \\ 0 \\ 0 \\ ... \\ 0 \end{matrix}
\right]\\ \left[
\begin{matrix} a_{11}-\lambda & a_{12} & a_{13} & ... & a_{1n} \\ a_{21} & a_{22}-\lambda & a_{23} & ... & a_{2n} \\ a_{31} & a_{32} & a_{33}-\lambda & ... & a_{3n} \\ ... \\ a_{n1} & a_{n2} & a_{n3} & ... & a_{nn}-\lambda \end{matrix}
\right] \cdot \left[
\begin{matrix} x_{1} \\ x_{2} \\ x_{3} \\ ... \\ x_{n} \end{matrix}
\right] &= \left[
\begin{matrix} 0 \\ 0 \\ 0 \\ ... \\ 0 \end{matrix}
\right] \\ (A - \lambda E) x &= 0 \end{aligned}$$ 



由行列式部分的知识: 齐次线性方程组有非零解(有唯一解)的充分必要条件是系数行列式的值为 0. 

$$\begin{aligned} D = \left|
\begin{matrix} a_{11}-\lambda & a_{12} & a_{13} & ... & a_{1n} \\ a_{21} & a_{22}-\lambda & a_{23} & ... & a_{2n} \\ a_{31} & a_{32} & a_{33}-\lambda & ... & a_{3n} \\ ... \\ a_{n1} & a_{n2} & a_{n3} & ... & a_{nn}-\lambda \end{matrix}
\right| &= 0 \end{aligned}$$ 

求得 $\lambda$ 的解, 再求 $x$, 即可得出矩阵 $A$ 的特征向量. 

$\lambda$ 可能有多个值, 都可以满足该方程 $D$, 即, 特征值有多个, 当矩阵不是满秩时, 非 $0$ 特征值可能不足 $n$ 个 (比如 $m$ 个), 不足的部分的特征值都可以认为是 $0$. 则该矩阵 $A$ 可以认为是将 $n$ 维的向量映射到 $m$ 维的子空间中. 



#### 行列式

用消元法解二元线性方程组: 

$$\begin{aligned} a_{11}x_{1} + a_{12}x_{2} &= b_{1} \\ a_{21}x_{1} + a_{22}x_{2} &= b_{2} \end{aligned}$$ 

消去未知数 $x_{2}$, 以 $a_{22}$ 与 $a_{12}$ 分别乘上列两方程的两端, 然后两方程相减, 得: 

$$\begin{aligned} (a_{11}a_{22} - a_{12}a_{21})x_{1} = b_{1}a_{22} - a_{12}b_{2} \end{aligned}$$ 

类似的: 

$$\begin{aligned} (a_{11}a_{22} - a_{12}a_{21})x_{2} = b_{2}a_{11} - a_{21}b_{1} \end{aligned}$$ 

上述用行列式表示: 



$$\begin{aligned} D &= a_{11}a_{22} - a_{12}a_{21} = \left|
\begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{matrix}
\right| \\ D_{1} &= b_{1}a_{22} - a_{12}b_{2} = \left|
\begin{matrix} b_{1} & a_{12} \\ b_{2} & a_{22} \end{matrix}
\right| \\ D_{2} &= b_{2}a_{11} - a_{21}b_{1} = \left|
\begin{matrix} a_{11} & b_{1} \\ a_{21} & b_{2} \end{matrix}
\right| \end{aligned}$$ 

方程的解为: 

$$\begin{aligned} x_{1} = \frac{D_{1}}{D} \\ x_{2} = \frac{D_{2}}{D} \end{aligned}$$ 

上式, 当 $D \ne 0$ 时, 又当 $b_{1} = b_{2} = 0$ 时, $x_{1} = x_{2} = 0$ . 即方程组的解为 $(0, 0)$. 但是当 $D=0$ 时, 方程组无解. **无解**, 其实是说无唯一解. 

此处不对 $n$ 元线性方程组的求解过程进行推导. 只是做个示例, 说明一下. 

所以, 以下方程组有非零解, 则须要方程组的系数项行列式为零. 

$$\begin{aligned}  \left[
\begin{matrix} (a_{11}-\lambda)x_{1} + a_{12}x_{2} + a_{13}x_{3} + ... + a_{1n}x_{n} \\ a_{21}x_{1} + (a_{22}-\lambda)x_{2} + a_{23}x_{3} + ... + a_{2n}x_{n} \\ a_{31}x_{1} + a_{32}x_{2} + (a_{33}-\lambda)x_{3} + ... + a_{3n}x_{n} \\ ... \\ a_{n1}x_{1} + a_{n2}x_{2} + a_{n3}x_{3} + ... + (a_{nn}-\lambda)x_{n} \end{matrix}
\right] &= \left[
\begin{matrix} 0 \\ 0 \\ 0 \\ ... \\ 0 \end{matrix}
\right]\\ \end{aligned}$$

即, 须要: 

$$\begin{aligned} D = \left|
\begin{matrix} a_{11}-\lambda & a_{12} & a_{13} & ... & a_{1n} \\ a_{21} & a_{22}-\lambda & a_{23} & ... & a_{2n} \\ a_{31} & a_{32} & a_{33}-\lambda & ... & a_{3n} \\ ... \\ a_{n1} & a_{n2} & a_{n3} & ... & a_{nn}-\lambda \end{matrix}
\right| &= 0 \end{aligned}$$

当 $D=0$ 时, 则矩阵 $A$ 有非零解. 



行列式的值: 

$$\begin{aligned} f(\lambda) = \left|
\begin{matrix} a_{11}-\lambda & a_{12} & a_{13} & ... & a_{1n} \\ a_{21} & a_{22}-\lambda & a_{23} & ... & a_{2n} \\ a_{31} & a_{32} & a_{33}-\lambda & ... & a_{3n} \\ ... \\ a_{n1} & a_{n2} & a_{n3} & ... & a_{nn}-\lambda \end{matrix}
\right| &= \sum(-1)^{t}a_{1p_{1}}a_{2p_{2}}a_{3p_{3}} \end{aligned}$$







#### 对称矩阵

对于任意的对称矩阵 $A_{n \times n}$ , 以上行列式 $D$ 都有解, 都可以进行特征值分解, 进行对角化. 即存在特征向量组 $X$ 使得: 

$$\begin{aligned} AX &= X \Lambda \\ A &= X \Lambda X^{-1} \\ A &= X \Lambda X^{T} \end{aligned}$$ 

其中: 

* $A$ 是任意 $n$ 阶对称矩阵. 
* $\Lambda $ 是矩阵 $A$ 的特征值组成的对角矩阵. 
* $X$ 是由特征向量组成的 $n$ 阶矩阵. 其中每一列是一个特征向量 (如果需要是`单位向量`, 须与 $\Lambda $ 做规一化). 







#### 应用

##### 1. 协方差矩阵对角化

设 $n$ 维空间中的 $m$ 个向量 $X_{m \times n}$ 在其各个维度上的方差都为 $1$, 现将其映射到该空间中的 $n$ 个单位向量 $A_{n \times n} $ 中,  $\Lambda_{n \times n}$ 是对单位向量长度进行缩放的系数项. 如下: 

$$\begin{aligned} X_{m \times n} A_{n \times n} \Lambda_{n \times n} = \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \left[ \begin{matrix} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \cdots & \cdots & \cdots & \cdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{matrix} \right] \left[ \begin{matrix} \lambda_{1} & 0 & \cdots & 0 \\ 0 & \lambda_{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \end{aligned}$$ 



求该矩阵的协方差矩阵: 

$$\begin{aligned} Y_{m \times n}^{\mathsf{T}} Y_{m \times n} = & \Lambda_{n \times n}^{\mathsf{T}} A_{n \times n}^{\mathsf{T}} X_{m \times n}^{\mathsf{T}} X_{m \times n} A_{n \times n} \Lambda_{n \times n} \\ = &  \left[ \begin{matrix} \lambda_{1} & 0 & \cdots & 0 \\ 0 & \lambda_{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \left[ \begin{matrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{matrix} \right] \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \left[ \begin{matrix} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \cdots & \cdots & \cdots & \cdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{matrix} \right] \left[ \begin{matrix} \lambda_{1} & 0 & \cdots & 0 \\ 0 & \lambda_{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right]  \\ = &  \left[ \begin{matrix} \lambda_{1} & 0 & \cdots & 0 \\ 0 & \lambda_{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \left[ \begin{matrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ a_{n1} & a_{n2} & \cdots & a_{nn} \end{matrix} \right] \left[ \begin{matrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & 1 \end{matrix} \right] \left[ \begin{matrix} a_{11} & a_{21} & \cdots & a_{n1} \\ a_{12} & a_{22} & \cdots & a_{n2} \\ \cdots & \cdots & \cdots & \cdots \\ a_{1n} & a_{2n} & \cdots & a_{nn} \end{matrix} \right] \left[ \begin{matrix} \lambda_{1} & 0 & \cdots & 0 \\ 0 & \lambda_{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \\ =& \left[ \begin{matrix} \lambda_{1}a_{11} & \lambda_{1}a_{12} & \cdots & \lambda_{1}a_{1n} \\ \lambda_{2}a_{21} & \lambda_{2}a_{22} & \cdots & \lambda_{2}a_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ \lambda_{n}a_{n1} & \lambda_{n}a_{n2} & \cdots & \lambda_{n}a_{nn} \end{matrix} \right] \left[ \begin{matrix} \lambda_{1}a_{11} & \lambda_{2}a_{21} & \cdots & \lambda_{n}a_{n1} \\ \lambda_{1}a_{12} & \lambda_{2}a_{22} & \cdots & \lambda_{n}a_{n2} \\ \cdots & \cdots & \cdots & \cdots \\ \lambda_{1}a_{1n} & \lambda_{2}a_{2n} & \cdots & \lambda_{n}a_{nn} \end{matrix} \right] \\ = & \left[ \begin{matrix} \lambda_{1}\lambda_{1}\sigma_{11} & \lambda_{1}\lambda_{2}\sigma_{12} & \cdots & \lambda_{1}\lambda_{n}\sigma_{1n} \\ \lambda_{2}\lambda_{1}\sigma_{21} & \lambda_{2}\lambda_{2}\sigma_{22} & \cdots & \lambda_{2}\lambda_{n}\sigma_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ \lambda_{n}\lambda_{1}\sigma_{n1} & \lambda_{n}\lambda_{2}\sigma_{n2} & \cdots & \lambda_{n}\lambda_{n}\sigma_{nn} \end{matrix} \right]   \end{aligned}$$ 

因此, 矩阵的协方差矩阵表达了将其变换的基向量的相关性及特征值. 

要求得变换矩阵 $B_{n \times n}$ 使得 $B_{n \times n}^{\mathsf{T}} Y_{m \times n}^{\mathsf{T}} Y_{m \times n} B_{n \times n}$ 是对角矩阵. 则只需 $A_{n \times n}B_{n \times n} = I_{n \times n}$, 实际上是求 $A_{n \times n}$ 的逆矩阵. 推导如下: 

$$\begin{aligned} B_{n \times n}^{\mathsf{T}} Y_{m \times n}^{\mathsf{T}} Y_{m \times n} B_{n \times n} = & B_{n \times n}^{\mathsf{T}} \Lambda_{n \times n}^{\mathsf{T}} A_{n \times n}^{\mathsf{T}} X_{m \times n}^{\mathsf{T}} X_{m \times n} A_{n \times n} \Lambda_{n \times n} B_{n \times n} \\ = & \Lambda_{n \times n}^{\mathsf{T}} B_{n \times n}^{\mathsf{T}} A_{n \times n}^{\mathsf{T}} X_{m \times n}^{\mathsf{T}} X_{m \times n} A_{n \times n} B_{n \times n} \Lambda_{n \times n} \\ = & \Lambda_{n \times n}^{\mathsf{T}} B_{n \times n}^{\mathsf{T}} A_{n \times n}^{\mathsf{T}} I_{n \times n} A_{n \times n} B_{n \times n} \Lambda_{n \times n} \\ = & \Lambda_{n \times n}^{\mathsf{T}} B_{n \times n}^{\mathsf{T}} A_{n \times n}^{\mathsf{T}} A_{n \times n} B_{n \times n} \Lambda_{n \times n} \\ = & \Lambda_{n \times n}^{\mathsf{T}} I_{n \times n} I_{n \times n} \Lambda_{n \times n} \\ = & \Lambda_{n \times n}^{\mathsf{T}} \Lambda_{n \times n} \\ = & \left[ \begin{matrix} \lambda_{1}^{2} & 0 & \cdots & 0 \\ 0 & \lambda_{2}^{2} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & \cdots & \lambda_{n}^{2} \end{matrix} \right] \end{aligned}$$ 



令: $A_{n \times n} = Y_{m \times n}^{\mathsf{T}} Y_{m \times n}$, $X_{n \times n} = B_{n \times n}$, $X_{n \times n}$ 是单位向量. 

$$\begin{aligned} X_{n \times n}^{\mathsf{T}} A_{n \times n} X_{n \times n} = & \Lambda \\ A_{n \times n} X_{n \times n} = & (X_{n \times n}^{\mathsf{T}})^{-1} \Lambda \\ A_{n \times n} X_{n \times n} = & X_{n \times n} \Lambda \\ A_{n \times n} X_{n \times n} = & \Lambda X_{n \times n} \end{aligned}$$ 





##### 2. PCA 降维

```text
https://blog.csdn.net/guoziqing506/article/details/80828165
```

如下图中的样本, PCA 想找到 $w_{1}$, $w_{2}$ 的基向量, 使得样本映射到该基向量后, 在其中 $w_{1}$ 上的分散程度最大, 这样就可以只取 $w_{1}$ 的特征作为样本描述, 以达到数据降维的效果. 

![image.png](https://s2.loli.net/2021/12/23/5qk3QJy9X6hA1ia.png)



设 $m$ 个 $n$ 维的数据样本 $X_{m \times n}$ (默认其已经过 "中心化" 处理, 即 $\sum_{i=1}^{m}{X_{i}=0}$), 求一个 $n$ 维单位向量 $W_{n \times 1}$, 使 $X_{m \times n}$ 投影到该向量后, 方差最大 (最为分散). 

$$\begin{aligned} \Sigma =& W_{1 \times n}^{\mathsf{T}} X_{m \times n}^{\mathsf{T}} X_{m \times n} W_{n \times 1} \\ =& \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right] \\ = & \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} \sum_{i=1}^{n}{w_{i}x_{1i}} \\ \sum_{i=1}^{n}{w_{i}x_{2i}} \\ \cdots \\ \sum_{i=1}^{n}{w_{i}x_{mi}} \end{matrix} \right] \\ = & \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11}\sum_{i=1}^{n}{w_{i}x_{1i}} + x_{21}\sum_{i=1}^{n}{w_{i}x_{2i}} + \cdots + x_{m1}\sum_{i=1}^{n}{w_{i}x_{mi}} \\ x_{12}\sum_{i=1}^{n}{w_{i}x_{1i}} + x_{22}\sum_{i=1}^{n}{w_{i}x_{2i}} + \cdots + x_{m2}\sum_{i=1}^{n}{w_{i}x_{mi}} \\ \cdots \\ x_{1n}\sum_{i=1}^{n}{w_{i}x_{1i}} + x_{2n}\sum_{i=1}^{n}{w_{i}x_{2i}} + \cdots + x_{mn}\sum_{i=1}^{n}{w_{i}x_{mi}} \end{matrix} \right] \\ = & \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} \sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) \\  \sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) \\ \cdots \\ \sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}) \end{matrix} \right] \\ = & w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \cdots + w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}) \\ = &  \sum_{k=1}^{n}(w_{k}\sum_{j=1}^{m}(x_{jk}\sum_{i=1}^{n}{w_{i}x_{ji}})) \\ = & \left[ \begin{matrix} \sum_{i=1}^{n}{w_{i}x_{1i}} & \sum_{i=1}^{n}{w_{i}x_{2i}} & \cdots & \sum_{i=1}^{n}{w_{i}x_{mi}} \end{matrix} \right] \cdot \left[ \begin{matrix} \sum_{i=1}^{n}{w_{i}x_{1i}} \\ \sum_{i=1}^{n}{w_{i}x_{2i}} \\ \cdots \\ \sum_{i=1}^{n}{w_{i}x_{mi}} \end{matrix} \right] \\ = & (\sum_{i=1}^{n}{w_{i}x_{1i}})^{2} + (\sum_{i=1}^{n}{w_{i}x_{2i}})^{2} + \cdots + (\sum_{i=1}^{n}{w_{i}x_{mi}})^{2}  \end{aligned}$$ 



其中: $W_{n \times 1}$ 是单位向量. 

$$\begin{aligned} \Sigma = & \sum_{k=1}^{n}(w_{k}\sum_{j=1}^{m}(x_{jk}\sum_{i=1}^{n}{w_{i}x_{ji}})) \\ 0 = & 1 - \sum_{i=1}^{n}{w_{i}^{2}} \end{aligned}$$ 



拉格朗日乘子法求 $\Sigma$ 的最大值. 

$$\begin{aligned} F(w_{1}, w_{2}, \cdots, w_{n}, \lambda) = \Sigma + \lambda (1 - \sum_{i=1}^{n}{w_{i}^{2}}) \end{aligned}$$ 



求导: 

$$\begin{aligned} \frac{\partial{F}}{\partial{w_{i}}} = & \frac{\partial{\Sigma}}{\partial{w_{i}}} - 2 \lambda w_{i} \quad i \in [1, n] \\ \frac{\partial{F}}{\partial{\lambda}} =& 1 - \sum_{i=1}^{n}{w_{i}^{2}} \end{aligned}$$ 



从 $\frac{\partial{F}}{\partial{w_{i}}} = \frac{\partial{\Sigma}}{\partial{w_{i}}} - 2 \lambda w_{i} = 0$ 求 $\lambda$. 

令 $w_{i} = w_{1}$: 

有: 

$$\begin{aligned} \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} = & \sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{1}(x_{11}x_{11} + x_{21}x_{21} + \cdots + x_{m1}x_{m1}) \\ = & \sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{1}\sum_{i=1}^{m}{x_{i1}x_{i1}} \end{aligned}$$ 



$$\begin{aligned} \frac{\partial{(w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} = & w_{2}(x_{12}x_{11} + x_{22}x_{21} + \cdots + x_{m2}x_{m1}) \\ = & w_{2} \sum_{i=1}^{m}{x_{i2}x_{i1}} \end{aligned}$$ 



$$\begin{aligned} \frac{\partial{(w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} = & w_{n}(x_{1n}x_{11} + x_{2n}x_{21} + \cdots + x_{mn}x_{m1}) \\ = & w_{n} \sum_{i=1}^{m}{x_{in}x_{i1}} \end{aligned}$$ 



因此有: 

$$\begin{aligned} \frac{\partial{\Sigma}}{\partial{w_{1}}} = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \cdots + w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} \\ = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} + \frac{\partial{(w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{1}}} + \cdots + \frac{\partial{(w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}})))}}{\partial{w_{1}}} \\ = & \sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{1}\sum_{i=1}^{m}{x_{i1}x_{i1}} + w_{2} \sum_{i=1}^{m}{x_{i2}x_{i1}} + \cdots + w_{n} \sum_{i=1}^{m}{x_{in}x_{i1}} \\ = & \sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \sum_{j=1}^{n}{(w_{j} \sum_{i=1}^{m}{x_{ij}x_{i1}})} \\ = & \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right] + \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} \\ x_{21} \\ \cdots \\ x_{m1} \end{matrix} \right] \\ = & 2 \times \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right]  \end{aligned}$$ 



$$\begin{aligned} \frac{\partial{\Sigma}}{\partial{w_{2}}} = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \cdots + w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{2}}} \\ = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{2}}} + \frac{\partial{(w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{2}}} + \cdots + \frac{\partial{(w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}})))}}{\partial{w_{2}}} \\ = & \sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{1}\sum_{i=1}^{m}{x_{i1}x_{i2}} + w_{2} \sum_{i=1}^{m}{x_{i2}x_{i2}} + \cdots + w_{n} \sum_{i=1}^{m}{x_{in}x_{i2}} \\ = & \sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \sum_{j=1}^{n}{(w_{j} \sum_{i=1}^{m}{x_{ij}x_{i2}})} \\ = & \left[ \begin{matrix} x_{12} & x_{22} & \cdots & x_{m2} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right] + \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{12} \\ x_{22} \\ \cdots \\ x_{m2} \end{matrix} \right] \\ = & 2 \times \left[ \begin{matrix} x_{12} & x_{22} & \cdots & x_{m2} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right]  \end{aligned}$$ 



$$\begin{aligned} \frac{\partial{\Sigma}}{\partial{w_{n}}} = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \cdots + w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{n}}} \\ = & \frac{\partial{(w_{1}\sum_{j=1}^{m}(x_{j1}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{n}}} + \frac{\partial{(w_{2}\sum_{j=1}^{m}(x_{j2}\sum_{i=1}^{n}{w_{i}x_{ji}}))}}{\partial{w_{n}}} + \cdots + \frac{\partial{(w_{n}\sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}})))}}{\partial{w_{n}}} \\ = & \sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}) + w_{1}\sum_{i=1}^{m}{x_{i1}x_{in}} + w_{2} \sum_{i=1}^{m}{x_{i2}x_{in}} + \cdots + w_{n} \sum_{i=1}^{m}{x_{in}x_{in}} \\ = & \sum_{j=1}^{m}(x_{jn}\sum_{i=1}^{n}{w_{i}x_{ji}}) + \sum_{j=1}^{n}{(w_{j} \sum_{i=1}^{m}{x_{ij}x_{in}})} \\ = & \left[ \begin{matrix} x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right] + \left[ \begin{matrix} w_{1} & w_{2} & \cdots & w_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{1n} \\ x_{2n} \\ \cdots \\ x_{mn} \end{matrix} \right] \\ = & 2 \times \left[ \begin{matrix} x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right]  \end{aligned}$$ 



则有: 

$$\begin{aligned} \frac{\partial{\Sigma}}{\partial{W}} = \left[ \begin{matrix} \frac{\partial{\Sigma}}{\partial{w_{1}}} \\ \frac{\partial{\Sigma}}{\partial{w_{2}}} \\ \cdots \\ \frac{\partial{\Sigma}}{\partial{w_{n}}} \end{matrix} \right] = & 2 \times \left[ \begin{matrix} x_{11} & x_{21} & \cdots & x_{m1} \\ x_{12} & x_{22} & \cdots & x_{m2} \\ \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & \cdots & x_{1n} \\ x_{21} & x_{22} & \cdots & x_{2n} \\ \cdots & \cdots & \cdots & \cdots \\ x_{m1} & x_{m2} & \cdots & x_{mn} \end{matrix} \right] \cdot \left[ \begin{matrix} w_{1} \\ w_{2} \\ \cdots \\ w_{n} \end{matrix} \right] \\ = & 2 X^{\mathsf{T}} X W \end{aligned}$$ 



拉格朗日乘子法求 $\Sigma$ 的最大值. 

$$\begin{aligned} F(w_{1}, w_{2}, \cdots, w_{n}, \lambda) = \Sigma + \lambda (1 - \sum_{i=1}^{n}{w_{i}^{2}}) \end{aligned}$$ 

导数: 

$$\begin{aligned} \frac{\partial{F}}{\partial{W}} = & \frac{\partial{\Sigma}}{\partial{W}} - 2 \lambda W \\ = & 2 X^{\mathsf{T}} X W - 2 \lambda W \end{aligned}$$ 



令 $\frac{\partial{F}}{\partial{W}} = 0$, 有: 

$$\begin{aligned} X^{\mathsf{T}} X W = \lambda W \end{aligned}$$ 



也就是说, 求令 $\Sigma$ 取得最大值是求解上式方程. 这刚好是**求解 $X^{\mathsf{T}} X$ 的特征值与特征向量**. 





##### 3. 图像处理 - 边

在图像处理中, 有角点, 边的识别. 在识别边时, 需要找到边的方向. 步骤如下: 

1. 针对一个点, 求出该点在两个方向上二阶导数 (曲率). 
2. 二阶导数, 实际上是类似于协方差矩阵的对称矩阵. 
3. 与边的方向垂直的方向就是曲率最大, 边的方向是曲率较小的方向. 且两个方向的曲率应不相关. 
4. 因此求边的方向, 就成了求二阶导数的特征向量的方向. 



##### 4. 信息压缩

有: 

$$\begin{aligned} A = & X \Lambda X^{-1} \\ = & X \Lambda X^{\top} \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix}
\right] \cdot \left[
\begin{matrix} x_{11} & x_{12} & x_{13} & \cdots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \cdots & x_{2n} \\ x_{31} & x_{32} & x_{33} & \cdots & x_{3n} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{n1} & x_{n2} & x_{n3} & \cdots & x_{nn} \end{matrix}
\right] \end{aligned}$$ 

通过上式可以看出, $Ax$ 运算, 相当于是将 $x$ 向量投影到特征向量 (基向量) 上, 再分别缩放 (乘以特征值) 再相加. 这样的变换过程. 



设有任意向量 $Y_{n \times 1}$, 

1. 将该向量 $Y_{n \times 1}$ 映射到基向量组上. 
2. 通过特征值对分解后的 $Y_{n \times 1}$ 进行缩放. 
3. 向量在各基向量上的投影值经过缩放后, 相加, 还原向量. 



$$\begin{aligned} AY = & X \Lambda X^{\top} Y \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix}
\right] \cdot \left[
\begin{matrix} x_{11} & x_{12} & x_{13} & \cdots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \cdots & x_{2n} \\ x_{31} & x_{32} & x_{33} & \cdots & x_{3n} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{n1} & x_{n2} & x_{n3} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} y_{1} \\ y_{2} \\ y_{3} \\ \cdots \\ y_{n} \end{matrix}
\right] \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[ \begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} x_{11} & x_{12} & x_{13} & \cdots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \cdots & x_{2n} \\ x_{31} & x_{32} & x_{33} & \cdots & x_{3n} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{n1} & x_{n2} & x_{n3} & \cdots & x_{nn} \end{matrix} \right] \cdot \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \gamma_{3} \\ \cdots \\ \gamma_{n} \end{matrix}
\right] \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[ \begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & 1 \end{matrix} \right] \cdot \left[
\begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \gamma_{3} \\ \cdots \\ \gamma_{n} \end{matrix}
\right] \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[ \begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix} \right] \cdot \left[ \begin{matrix} \gamma_{1} \\ \gamma_{2} \\ \gamma_{3} \\ \cdots \\ \gamma_{n} \end{matrix} \right] \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[ \begin{matrix} \lambda_{1} \gamma_{1} \\ \lambda_{2} \gamma_{2} \\ \lambda_{3} \gamma_{3} \\ \cdots \\ \lambda_{n} \gamma_{n} \end{matrix} \right] \\ = & \left[
\begin{matrix} \lambda_{1} \gamma_{1} x_{11} + \lambda_{2} \gamma_{2} x_{21} + \lambda_{3} \gamma_{3} x_{31} + \cdots + \lambda_{n} \gamma_{n} x_{n1} \\ \lambda_{1} \gamma_{1} x_{12} + \lambda_{2} \gamma_{2} x_{22} + \lambda_{3} \gamma_{3} x_{32} + \cdots + \lambda_{n} \gamma_{n} x_{n2} \\ \lambda_{1} \gamma_{1} x_{13} + \lambda_{2} \gamma_{2} x_{23} + \lambda_{3} \gamma_{3} x_{33} + \cdots + \lambda_{n} \gamma_{n} x_{n3} \\ \cdots \\ \lambda_{1} \gamma_{1} x_{1n} + \lambda_{2} \gamma_{2} x_{2n} + \lambda_{3} \gamma_{3} x_{3n} + \cdots + \lambda_{n} \gamma_{n} x_{nn} \end{matrix}
\right] \end{aligned}$$ 

计算第 $i$ 个基向量对结果的影响: 

$$\begin{aligned} \text{square_error} = & \left[ \begin{matrix} \lambda_{i} \gamma_{i} x_{i1} & \lambda_{i} \gamma_{i} x_{i2} & \lambda_{i} \gamma_{i} x_{i3} & \cdots & \lambda_{i} \gamma_{i} x_{in} \end{matrix} \right] \cdot \left[
\begin{matrix} \lambda_{i} \gamma_{i} x_{i1} \\ \lambda_{i} \gamma_{i} x_{i2} \\  \lambda_{i} \gamma_{i} x_{i3} \\ \cdots \\ \lambda_{i} \gamma_{i} x_{in} \end{matrix}
\right] \\ = & \lambda_{i}^{2} \times \gamma_{i}^{2} \times \left[ \begin{matrix}  x_{i1} & x_{i2} & x_{i3} & \cdots & x_{in} \end{matrix} \right] \cdot \left[
\begin{matrix} x_{i1} \\ x_{i2} \\ x_{i3} \\ \cdots \\ x_{in} \end{matrix}
\right] \\ = & \lambda_{i}^{2} \times \gamma_{i}^{2} \times 1 \end{aligned}$$ 

上式, 对于非特定向量, $\gamma_{i}^{2}$ 的期望 $E(\gamma_{i}^{2}) = C$  是常量. 因此, 特征向量对 **平方误差** 的影响是由特征值大小决定的. 

在实践中, 可舍弃特征值较小的特征向量, 以实现数据压缩. 



**信息压缩实例**: 

令 $I$ 表示单位向量. 

$$\begin{aligned} A = & A I \\ = &  X \Lambda X^{\top} I \\ = & \left[
\begin{matrix} x_{11} & x_{21} & x_{31} & \cdots & x_{n1} \\ x_{12} & x_{22} & x_{32} & \cdots & x_{n2} \\ x_{13} & x_{23} & x_{33} & \cdots & x_{n3} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{1n} & x_{2n} & x_{3n} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} \lambda_{1} & 0 & 0 & \cdots & 0 \\ 0 & \lambda_{2} & 0 & \cdots & 0 \\ 0 & 0 & \lambda_{3} & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & \lambda_{n} \end{matrix}
\right] \cdot \left[
\begin{matrix} x_{11} & x_{12} & x_{13} & \cdots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \cdots & x_{2n} \\ x_{31} & x_{32} & x_{33} & \cdots & x_{3n} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ x_{n1} & x_{n2} & x_{n3} & \cdots & x_{nn} \end{matrix}
\right] \cdot \left[
\begin{matrix} 1 & 0 & 0 & \cdots & 0 \\ 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 & \cdots & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & 1 \end{matrix}
\right]  \end{aligned}$$ 

根据前面的推导, 当我们在此处位留全部的单位基向量时, 可以完全还原矩阵 $A$. 

当舍弃值最小的 $\lambda$ 时, 可得到与原矩阵 $A$ 相比 $\text{square_error}$ 平方误差最小的近似矩阵 $A^{'}$. 



































