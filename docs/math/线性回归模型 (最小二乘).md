## 线性回归模型 (最小二乘)



### OLS 普通最小二乘. 

给定一组观测样本 $(y_{1}, y_{2}, \cdots , y_{T})$, $\beta$ 的普通最小二乘 (OLS) 估计 (记作 $\hat{\beta}$) 是使得残差平方和 (RSS) 最小化值, $\beta$ 的值: 

$$\begin{aligned} RSS = \sum_{t=1}^{T}{(y_{t} - x_{t}^{T}\beta)^{2}}. \end{aligned}$$ 



假设真实的数据由下式表示: 

$$\begin{aligned} Y = X \beta + u. \end{aligned}$$

其中: 

* Y: 是因变量. 
* X: 是自变量. 
* $\beta$ 是系数项. 
* $u$ 是残差 (误差) 项. 因注意这是真实数据固有的残差. 



通过最小二乘法求 RSS 的最小化, 我们知道 $\beta$ 的估计值为: $\hat{\beta} = (X^{T}X)^{-1}X^{T}Y$. 因此, 我们估计出的系数项 $\beta$ 对应的因变量 Y 各项的残差为: 

$$\begin{aligned} \hat{u} &= Y - X \hat{\beta} \\ &= Y - X(X^{T}X)^{-1}X^{T} Y \\ &= (I - X(X^{T}X)^{-1}X^{T})Y \end{aligned}$$ 



将: 

$$\begin{aligned} Y = X \beta + u. \end{aligned}$$ 

代入上式: 

$$\begin{aligned} \hat{u} &= (I - X(X^{T}X)^{-1}X^{T})Y \\ &= (I - X(X^{T}X)^{-1}X^{T})(X \beta + u) \\ &= (I - X(X^{T}X)^{-1}X^{T})X \beta + (I - X(X^{T}X)^{-1}X^{T})u \\ &= (X \beta - X(X^{T}X)^{-1}X^{T}X \beta) + (I - X(X^{T}X)^{-1}X^{T})u \\ &= (X \beta - X(X^{T}X)^{-1}(X^{T}X) \beta) + (I - X(X^{T}X)^{-1}X^{T})u \\ &= (X \beta - X \beta) + (I - X(X^{T}X)^{-1}X^{T})u \\ &= (I - X(X^{T}X)^{-1}X^{T})u  \end{aligned}$$ 



#### 估计的系数 $\hat{\beta}$ 与真实系数 $\beta$ 的关系. 

$$\begin{aligned} \hat{\beta} &= (X^{T}X)^{-1}X^{T}Y \\ &= (X^{T}X)^{-1}X^{T}(X \beta + u) \\ &= (X^{T}X)^{-1}X^{T}X\beta + (X^{T}X)^{-1}X^{T}u \\ &= (X^{T}X)^{-1}(X^{T}X)\beta + (X^{T}X)^{-1}X^{T}u \\ &= \beta + (X^{T}X)^{-1}X^{T}u \end{aligned}$$ 



假设: 真实残差 $u$ 满足: $E(u)=0$, $E(uu^{T}) = \sigma^{2}I_{T}$, 均值为 0, 独立. 

则有, 期望: 

$$\begin{aligned} E(\hat{\beta}) &= E[\beta + (X^{T}X)^{-1}X^{T}u] \\ &= \beta + (X^{T}X)^{-1}X^{T}E(u) \\ &= \beta \end{aligned}$$ 



协方差矩阵: 

$$\begin{aligned} E[(\hat{\beta} - \beta) \cdot (\hat{\beta} - \beta)^{T}] &= E\{[(X^{T}X)^{-1}X^{T}u] \cdot [(X^{T}X)^{-1}X^{T}u]^{T} \} \\ &= E[(X^{T}X)^{-1}X^{T}uu^{T}X(X^{T}X)^{-1}] \\ &= (X^{T}X)^{-1}X^{T}E[uu^{T}]X(X^{T}X)^{-1} \\ &= (X^{T}X)^{-1}X^{T} \sigma^{2}I_{T} X(X^{T}X)^{-1} \\ &= \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\ &= \sigma^{2}(X^{T}X)^{-1}(X^{T}X)(X^{T}X)^{-1} \\ &= \sigma^{2}(X^{T}X)^{-1} \end{aligned}$$ 

备注: 

* $X^{T}X$ 是对称矩阵, 它的逆矩阵 $(X^{T}X)^{-1}$ 也是对称矩阵. 



结论: 

* 当假设满足时, (真实残差 $u$ 满足: $E(u)=0$, $E(uu^{T}) = \sigma^{2}I_{T}$, 均值为 0, 独立), 普通最小二乘法估计出的残差是真实残差的**无偏估计**. 





### GLS 广义最小二乘

考察: 当真实残差 $u$ 不满足独立性假设时, 即其协方差不是对角矩阵而是对称矩阵时. 有 $E(u)=0$, $E(uu^{T}) = \sigma^{2}V_{T \times T}$. 



对于对称正定矩阵 $V_{T \times T}$ 存在一个非奇异矩阵 $L_{T \times T}$ 使得: 

$$\begin{aligned} V^{-1} = L^{T}L \end{aligned}$$



对真实残差 $u$ 做 $L$ 变换: 

$$\begin{aligned} \widetilde{u} \equiv Lu \end{aligned}$$ 

则真实数据表示: 

$$\begin{aligned} Y &= X \beta + u \\ LY &= LX\beta + Lu \\ \widetilde{Y} &= \widetilde{X} \beta + \widetilde{u} \end{aligned}$$ 

其中: $\widetilde{u} \equiv Lu$ 满足: $E(u)=0$, $E(uu^{T}) = \sigma^{2}I_{T}$, 均值为 0, 独立. 因此, 变换后的模型就满足了 OLS 普通最小二乘中的假设. 



有, 估计的系数项 $\hat{\beta} $ 为: 

$$\begin{aligned} \hat{\beta} &= (\widetilde{X}^{T}\widetilde{X})^{-1}\widetilde{X}^{T}\widetilde{Y} \\ &= (X^{T}L^{T}LX)^{-1}X^{T}L^{T}LY \\ &= (X^{T}V^{-1}X)^{-1}X^{T}V^{-1}Y \end{aligned}$$ 

同 OLS 中的推导. 估计出的残差是真实残差的**无偏估计**. 

$$\begin{aligned} E(\hat{\beta}) &= \beta \end{aligned}$$ 

$$\begin{aligned} E[(\hat{\beta} - \beta) \cdot (\hat{\beta} - \beta)^{T}] &= \sigma^{2}(\widetilde{X}^{T}\widetilde{X})^{-1} \\ &= \sigma^{2}(X^{T} V^{-1} X)^{-1} \end{aligned}$$ 



#### 自相关

考虑残差项满足: 

$$\begin{aligned} u_{t} = \rho u_{t-1} + \varepsilon_{t}. \end{aligned}$$ 

其中 $| \rho | < 1$ 且 $\varepsilon^{t}$ 是高斯分布的白噪声, 方差为 $\sigma^{2}$. 于是: 

 $$\begin{aligned} E(uu^{T}) &= \frac{\sigma^{2}}{1 - \rho^{2}} \left[
\begin{matrix} 1 & \rho & \rho^{2} & \cdots & \rho^{T-1} \\ \rho &  1 & \rho & \cdots & \rho^{T-2} \\ \rho^{2} & \rho &  1 & \cdots & \rho^{T-2} \\ \cdots & \cdots & \cdots & \cdots & \cdots \\ \rho^{T-1} & \rho^{T-2} & \rho^{T-3} & \cdots & 1 \end{matrix} \right] \\ &= \sigma^{2}V \end{aligned}$$ 



$$\begin{aligned} L &= \left[ \begin{matrix} \sqrt{1 - \rho^{2}} & 0 & 0 & \cdots & 0 & 0 \\ -\rho &  1 & 0 & \cdots & 0 & 0 \\ 0 & -\rho &  1 & \cdots & 0 & 0 \\ \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ 0 & 0 & 0 & \cdots & -\rho & 1 \end{matrix} \right] \end{aligned}$$ 



因此, GLS 估计可以由 $\widetilde{Y} = LY$ 对 $\widetilde{X} = LX$ 做 OLS 回归得到; 即用 $y_{1}\sqrt{1 - \rho^{2}}$ 对 $x_{1}\sqrt{1 - \rho^{2}}$ 回归, 关且对 $t=2, 3, \cdots , T$, 用 $y_{t} - \rho y_{t-1}$ 对 $x_{t} - \rho x_{t-1}$ 做回归. 





































