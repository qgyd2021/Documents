## 最大似然估计与交叉熵

https://blog.csdn.net/muyuu/article/details/110387254



### 最小编码长度





### 交叉熵

有两个概率分布 $P$, $Q$, 其中 $P$ 为真实分布,  $Q$ 为预测的分布. 信息论指出, 按分布 $P$ 来识别一个样本所需的平均编码长度等于该分布的熵: 

$$\begin{aligned} H(P) = - \sum_{i}{P(x_{i}) \log{P(x_{i})}} \end{aligned}$$ 

而由于现实当中, 我们很难知道训练数据的真实分布 $P$, 而如果用预测分布 $Q$ 来做同样的事情, 所需平均编码长度为: 

$$\begin{aligned} H(P, Q) = - E_{x \sim P} [\log{Q(x)}] = - \sum_{i}{P(x_{i}) \log{Q(x_{i})}} \end{aligned}$$ 





### 相对熵 (KL 散度)

相对熵的定义就是我们上面说的用预测分布 $Q$ 所需的平均编码长度比用真实分布 $P$ 所需的平均编码长度多多少. 所以说相对熵可以用来衡量两个分布的差距, 最小化相对熵就可以使得预测分布 $Q$ 尽量逼近真实分布 $P$, 因此可以使用相对熵来作为机器学习的目标函数. 

$$\begin{aligned} KL(P || Q) = H (P,Q) - H(P) = \sum_{i}{P(x_{i}) \log{\frac{P(x_{i})}{Q(x_{i})}}} \end{aligned}$$ 

但观察相对熵的公式, 我们可以发现 $H(P)$ 其实是一个固定的量 (尽管未知), 因此: 

$$\begin{aligned} \text{argmin}_{Q} KL (P || Q) = \text{argmin}_{Q} H(P,Q) \end{aligned}$$ 

最小化 KL 散度, 等价于最小化交叉熵. 所以交叉也可以作为机器学习的目标函数. 





### 最大似然估计

最大似然估计, $P$ 为真实分布, $Q$ 为预测分布, 但因为我们只知道预测分布, 所以用 $Q$ 来求最大似然: 

$$\begin{aligned} \theta^{*} = & \text{argmax}_{\theta} \prod_{i}{Q(x_{i}; \theta)} \\ = & \text{argmax}_{\theta} \sum_{i} {\log{Q(x_{i}; \theta)}} \\ = & \text{argmax}_{\theta} \frac{1}{m} \sum_{i} {\log{Q(x_{i}; \theta)}} \end{aligned}$$ 

当 $m$ 足够大时, 由大数定律: 

$$\begin{aligned} \frac{1}{m} \sum_{i} {\log{Q (x_{i}; \theta)}} \rightarrow E_{x - P}[\log{Q(x_{i};\theta)}] \end{aligned}$$ 

因此, 

$$\begin{aligned} \theta^{*} = & \text{argmax}_{\theta} \sum_{i} {P(x_{i}) \log{Q(x_{i}; \theta)}} \\ = & \text{argmax}_{\theta} - \sum_{i} {P(x_{i}) \log{Q(x_{i}; \theta)}} \\ = & \text{argmax}_{\theta} H(P,Q) \end{aligned}$$ 

总结: 当数据量足够大时, 最大似然估计也等价于最小化交叉熵. 

















