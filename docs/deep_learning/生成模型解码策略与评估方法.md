## 生成模型解码策略与评估方法



参考链接: 

```text
https://blog.csdn.net/weixin_43499457/article/details/126769391
https://zhuanlan.zhihu.com/p/587684961
https://github.com/huggingface/transformers/tree/main/src/transformers/generation


```





### 解码策略

is_constraint_gen_mode

is_contrastive_search_gen_mode

is_greedy_gen_mode

is_sample_gen_mode

is_beam_gen_mode
is_beam_sample_gen_mode
is_group_beam_gen_mode





#### 贪婪搜索解码

greedy search

```text
参考链接: 
https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py:line:2037 (gready_search)

```

贪心搜索 (greedy search) 在每个时间步 $t$ 都选取当前概率分布中概率最大的词, 即: 

$$\begin{aligned} \hat{y}_{t} = \underset {y} {\text{argmax}} \space P(y | \hat{Y}_{<t}, X) \end{aligned}$$ 

直到 $\hat{y}_{t}$ 为 $\text{<EOS>}$ 或达到预设最大长度时停止生成. 



贪心搜索本质上是**局部最优**策略, 但并不能保证最终结果一定是全局最优的. 

由于含心搜索在解码的任意时刻保留一条候选序列, 所以在搜索效率上, 贪心搜索的复杂度显著低于穷举搜索. 





#### 集束搜索解码

```text
参考链接: 
https://github.com/huggingface/transformers/blob/main/src/transformers/generation/beam_search.py
```



集束搜索 (beam search) 扩大了搜索范围, 对贪心搜索进行了有效改进. 虽然集束搜索的搜索范围远远不及穷举搜索, 但已经覆盖了大部分概率较高的文本, 因此在搜索方法中被广泛使用. 集束搜索有一个关键的超参数 "束宽" (beam size), 一般用 B 表示. 



集束搜索的基本流程是: 

1. 在第一个时间步, 选取当前概率最大的 B 个词, 分别当成 B 个候选输出序列的第一个词; 
2. 在之后的每个时间步, 将上一时刻的输出序列与词表中每个词组合后得到概率最大的 B 个扩增序列作为该时间步的候选输出序列. 

形式化地, 假设在 $t-1$ 时刻, B 个序列的集合表示为: 

$$\begin{aligned} \mathcal{Y}_{[t-1]} = \{ Y_{[t-1]}^{1}, \cdots , Y_{[t-1]}^{B} \} \end{aligned}$$ 

在 $t$ 时刻, 集束搜索需要考虑所有这些集束与词表上所有单词的组合, 即在集合: 

$$\begin{aligned} \mathcal{S}_{t} = \{ (Y_{[t-1]}^{b}, y_{t}) | \forall (Y_{[t-1]}^{b} \in \mathcal{Y}_{[t-1]}) \and (y_{t} \in \mathcal{V} \cup \{ \text{<EOS>} \}) \} \end{aligned}$$

中保留 B 个概率最高的扩展作为 $\mathcal{Y}_{[t]}$, 其中: $(Y_{[t-1]}^{b}; y_{t})$ 表示将单词 $y_{t}$ 作为序列 $Y_{[t-1]}^{b}$ 的后缀与之拼接. 若 $y_{t} = \text{<EOS>}$ , 则表明相应的候选序列在此结束生成. 

具体来说, $\mathcal{Y}{[t]}$ 的更新公式为: 

$$\begin{aligned} \mathcal{Y}_{[t]} = & \underset {Y_{[t]}^{1}, \cdots , Y_{[t]}^{B} \in \mathcal{S}_{t}} {\text{argmax}} \sum_{b=1}^{B}{\log{ P(Y_{[t]}^{b} | X) }} \\ s.t. Y_{[t]}^{i} \ne & Y_{[t]}^{j}, \space \forall i \ne j, \space i,j=1,2,\cdots,B  \end{aligned}$$ 

重复上述步骤, 直至最大长度为 T, 最终得到 B 个候选序列 (使用对数将条件概率的乘法转换为加法是为了避免数据下溢). 



由于每一步生成的概率介于 0, 1 之间, 所以候选序列的生成概率随着不断累乘会越来越小, 因此, 集束搜索常常会倾向于生成较短的序列, 即较早地生成 $\text{<EOS>}$. 为了改进这个问题, 在对候选序列排序的过程中, 可以引入长度惩罚. 最简单的方法是使用长度归一化的条件概率, 即把每一个候选序列的概率除以它的**序列长度 n **后再进行排序. 实践中, 常常会给归一化因子 n 加上一个可调节参数 a 作为指数, 即 $n^{a}, a \in [0, 1]$. 当 $a=0$ 时, 不进行长度惩罚; 当 $a=1$ 时, 则直接用长度 $n$ 来进行惩罚. 上述过程可形式化地表示为: 

$$\begin{aligned} \mathcal{Y}_{[t]} = & \underset {Y_{[t]}^{1}, \cdots , Y_{[t]}^{B} \in \mathcal{S}_{t}} {\text{argmax}} \sum_{b=1}^{B}{ \frac{1}{(n_{b,[t]})^{a}} \log{ P(Y_{[t]}^{b} | X) }} \\ s.t. Y_{[t]}^{i} \ne & Y_{[t]}^{j}, \space \forall i \ne j, \space i,j=1,2,\cdots,B  \end{aligned}$$ 

其中, $n_{b,[t]}$ 表示候选序列 $Y_{[t]}^{b}$ 的长度. 



贪心搜索可以看作是 B=1 的集束搜索, 集束搜索是一种牺牲时间换性能的方法. 无论是贪心搜索还是集束搜索, 都给予最大似然的搜索目标, 即要求生成的文本有最高的概率. 但在实验中发现, 这类解码方法**容易生成重复的文本**. 一种可能的解释是, 模型在训练时解码器输入的每个位置都是人写的真实文本, 但在生成时每个位置的输入则是模型之前生成的文本. 由于人类文本并非总是在文本序列的每个位置上都取最高概率的词, 所以生成时如果仍然以最大概率为解码目标, 就导致训练和解码时进行概率预测的前缀输入在分布上存在差异, 可能造成不断生成重复片段的现象. 





#### 采样解码



**随机采样** 在生成时的每一步都从当前概率分布 $P(y | Y_{<t}, X)$ 中按照概率随机采样一个词, 即: 

$$\begin{aligned} \hat{y}_{t} \sim P(y | \hat{Y}_{<t}, X) \end{aligned}$$ 

相比基于搜索的解码方法, 通过采样生成的文本通常具有更高的多样性, 同时也在一定程度上缓解了生成通用和重复文本的问题. 





**带温度的随机采样** 尽管随机采样在一定程度上能避免生成重复的文本, 但是, 由于从词表中采样可能会采到与上下文无关的词, 因此, 随机采样得到的文本上下文常常不连贯. 

为了使得模型尽可能避免采样到低概率的词, 一个有效的办法是设置一个名为 "温度" (temperature) 的参数来控制概率分布的弥散程度, 该参数用 $\tau$ 表示, $\tau$ 是一个大于 0 的实数. 形式化地说, 生成过程中需要将概率分布的计算方法修改为: 

$$\begin{aligned} P(y | \hat{Y}_{<t}, X) = \text{softmax} (\text{MLP}(\hat{s}_{t} / \tau)) |_{y} \end{aligned}$$ 

* 当 $\tau=1$ 时, 即为原始的概率分布; 
* 当 $\tau<1$ 时, 得到的概率分布将更加尖锐, 弥散程度更小, 采样的随机性降低; 

* 当 $\tau \rightarrow 0$ 时, 使用随机采样解码的效果近似于贪婪搜索; 
* 当 $\tau > 1$ 时, 得到的概率分布弥散程度更小, 采样的随机性升高; 
* 当 $\tau \rightarrow \infty$ 时, 使用随机采样解码的效果则近似于从均匀分布中随机采样. 

因此, 合理设置 $\tau \in (0,1)$ 可以避免随机采到概率较小的词. 

 



**Top-k 采样** 除了设置温度来调整概率分布的弥散程度,Top-k 采样近来也被广泛使用. 具体来说, 在每个时间步, 解码器首先选择概率最高的 k 个词作为候选词, 然后根据 k 个词的相对概率大小从中采出一个词作为要生成的词. 

形式化地, 设在 t 时刻模型预测的在词表 $\mathcal{V}$ 上的概率分布为 $P(y| \hat{Y}_{<t}, X)$, 则它的 Top-k 词表为: 

$$\begin{aligned} \mathcal{V}^{(k)} = \text{argmax}_{\mathcal{V}^{'(k)}} \sum_{y \in \mathcal{V}^{'(k)}} {P(y | \hat{Y}_{<t},X)} \end{aligned}$$ 

其中 $\mathcal{V}^{'(k)} \subset \mathcal{V}$ , 则最初的概率分布被重新调整为一个新的分布. 

$$\begin{aligned} \tilde{P}(y_{t} | \hat{Y}_{<t}, X) = \left \{ \begin{matrix}\frac{ P(y_{t} | \hat{Y}_{<t}, X) }{\sum_{y \in \mathcal{V}^{(k)}} P(y | \hat{Y}_{<t}, X)}, \quad & y_{t} \in \mathcal{(k)} \\ 0, \quad & \text{otherwise} \end{matrix} \right . \end{aligned}$$ 

接着, 从 $\tilde{P}(y_{t} | \hat{Y}_{<t}, X)$ 中按概率进行随机采样, 得到要生成的词 $\hat{y}_{t}$ , 进而输入模型中继续进行下一步的生成. 





**Top-p 采样** 尽管 Top-k 采样已经能够显著提高文本生成的质量, 但是对于不同的模型, 常数 k 难以进行一致的设定. 在概率分布比较平坦的情况下, 词表中的有几百个词概率都相差不大, 意味着此时当前词的可能选择非常多, 可能存在超过 k 个合理的词. 这时如果限制仅仅从 Top-k 个候选词中采样, 可能会增加生成重复文本的风险. 同时, 如果概率分布非常集中, 意味着此时可选择的词数目非常少, 如可选的词汇少于 k 个, 则从 Top-k 个候选词中采样可能会采到上下文无关的词. 

相比于 Top-k 方法从概率最高的 k 个候选词中采样, Top-p 方法将其采样范围修改为 $\mathcal{V}^{(p)}$, $\mathcal{V}^{(p)}$ 是满足: 

$$\begin{aligned} \sum_{y \in \mathcal{V}^{'(p)}} {P(y | \hat{Y}_{<t}, X)} \ge p \end{aligned}$$

的所有 $\mathcal{V}^{'(p)}$ 中最小的集合, 其中 $p \in (0, 1)$ 是预选设定的超参数. Top-p 采样根据生成概率从高到低在词表上选择累积概率恰好超过 p 的候选词作为采样集合, 再从这些候选词中采样出最终的结果. 







































